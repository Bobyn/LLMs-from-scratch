{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3d2d12-1d70-4068-bfc7-493481b43fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numbers of chars: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "\"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "\"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total numbers of chars: {len(raw_text)}\")\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d7d7b7-52e1-470d-ac93-9569bb76b0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b1e2a4-6dad-451d-b9fe-06710708b7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4690\n",
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "file_path = \"ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "text = \"Hello, world. This-- is a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n",
    "# print(preprocessed[:30])\n",
    "# print(result)\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(f\"{item}\")\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd43ce4-ee2b-4197-8d86-fa8be2d50818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70455a8a-0dc3-41c9-a4e9-ed37dcfd6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "286143d8-dcfb-4aa6-888a-f186621d61d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
      "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"\"\"\"It's the last he painted, you know,\"Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)\n",
    "\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2af885-d9d7-4494-9899-72962181eb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3438dab3-bf04-4e6f-a6b5-1436f93bbc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n",
      "('younger', 1127)\n",
      "('your', 1128)\n",
      "('yourself', 1129)\n",
      "('<|endoftext|>', 1130)\n",
      "('<|unk|>', 1131)\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))\n",
    "\n",
    "for i, item in enumerate(list(vocab.items())[-5:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018fa723-adb2-4199-b46e-b0b6cc03713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1357788b-2375-4499-a048-e4b0f9825c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
      "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
      "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)\n",
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb715383-397b-4bb7-b64b-8137423439d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d213aaa4-7b1e-4a35-bcc1-eb3a10429a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.5 Byte pair encoding ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a19aa0-6af4-4f81-a93e-62bc38592880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "import tiktoken\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a789ccc-5f26-41e6-a2ae-4c09f7551fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "text = (\n",
    "\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\"\n",
    ")\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed390b5c-5c71-4873-9851-86c8e78ccaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9400e7dc-3cca-4f70-82b2-b0164a969bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n"
     ]
    }
   ],
   "source": [
    "unknown_text = \"Akwirw ier\"\n",
    "unk_ids = tokenizer.encode(unknown_text)\n",
    "print(unk_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8a6727-884b-475e-9a63-2bf3bbf85363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(unk_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c1c3a-db7d-4958-baea-23e3234e3d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36125694-4e1e-4662-b35e-bb0cd11e9be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.6 Data sampling with a sliding window ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b08fee5c-c0f7-49bc-90bf-bddb76d70e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "file_path = \"ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beb3ddd3-1fcf-4ffe-968e-1ae17dce0efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5095\n",
      "x: [290, 4920, 2241, 287]\n",
      "y: \t[4920, 2241, 287, 257]\n",
      "[290] ----> 4920\n",
      "[290, 4920] ----> 2241\n",
      "[290, 4920, 2241] ----> 287\n",
      "[290, 4920, 2241, 287] ----> 257\n"
     ]
    }
   ],
   "source": [
    "enc_sample = enc_text[50:]\n",
    "print(len(enc_sample))\n",
    "\n",
    "context_size = 4 #Determines how many ids are included in the input\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size + 1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: \\t{y}\")\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50442d5e-a4f6-4943-8dbd-26904cc0d4f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---->  established\n",
      " and established ---->  himself\n",
      " and established himself ---->  in\n",
      " and established himself in ---->  a\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993665a9-4259-45da-80b9-f756a2af8dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754ebad3-722c-4611-b1d6-4ea91f4c1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a39df19b-9e1d-4a0b-b56b-2ea2f8c99641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0f6eba6-083a-42c5-a447-2a797b8d5952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs 0:\n",
      "tensor([[   40,   367,  2885,  1464],\n",
      "        [  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257]])\n",
      "Targets 0:\n",
      "tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 2885,  1464,  1807,  3619],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [  402,   271, 10899,  2138],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [10899,  2138,   257,  7026]])\n"
     ]
    }
   ],
   "source": [
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "token_ids_aux = tokenizer.encode(raw_text)\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=1, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "# print(f\"TOKENS:\")\n",
    "# for i in range (0, 200, 4):\n",
    "#     print(f\"{token_ids_aux[i:i+4]}\")\n",
    "num_batches=1\n",
    "for i in range(0, num_batches):\n",
    "    inputs, targets = next(data_iter)\n",
    "    print(f\"Inputs {i}:\\n{inputs}\")\n",
    "    print(f\"Targets {i}:\\n{targets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8930cc-e86c-4718-a96a-be8b81b22c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "262d30c9-fd76-40fe-a89e-0b5497477fe6",
   "metadata": {},
   "source": [
    "## 2.7 Creating token embeddings ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0602df80-9b36-492c-a0a3-8827fd96a0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([2,3,5,1])\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd783d74-2787-45c4-bcd5-779da010509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.8400, -0.7849, -1.4096]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f7de0a3-29a9-4e64-a34e-4c94d6d57f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1eb4b04b-8dca-4183-982d-3237517eee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
    "# print(\"\\nTargets shape:\\n\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "111a9183-8367-447d-b2f8-2201dc765cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b65a756-82ce-4334-a06e-cb0242d3a773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n",
      "tensor([[ 1.7375, -0.5620, -0.6303,  ..., -0.2277,  1.5748,  1.0345],\n",
      "        [ 1.6423, -0.7201,  0.2062,  ...,  0.4118,  0.1498, -0.4628],\n",
      "        [-0.4651, -0.7757,  0.5806,  ...,  1.4335, -0.4963,  0.8579],\n",
      "        [-0.6754, -0.4628,  1.4323,  ...,  0.8139, -0.7088,  0.4827]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
    "print(pos_embeddings.shape)\n",
    "print(pos_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "505f4b61-c615-4689-84c1-1dfee3245588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n",
      "Tokens_embedding: tensor([-1.8622, -0.1914, -0.3812], grad_fn=<SliceBackward0>)\n",
      "Pos_embeddings: tensor([ 1.6423, -0.7201,  0.2062], grad_fn=<SliceBackward0>)\n",
      "Input_embeddings: tensor([-0.2199, -0.9114, -0.1750], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "print(f\"Tokens_embedding: {token_embeddings[1, 1, :3]}\")\n",
    "print(f\"Pos_embeddings: {pos_embeddings[1, :3]}\")\n",
    "print(f\"Input_embeddings: {input_embeddings[1, 1, :3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5d966-2d9b-4883-a821-82ad00a1585e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0a26c5-0beb-44c1-8ff8-6594da8da78e",
   "metadata": {},
   "source": [
    "# CHAPTER 3 - Coding attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d8a15-db1f-40d5-a66d-69900e317c04",
   "metadata": {},
   "source": [
    "### 3.3.1 A simple self-attention mechanism without trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d1abee4-dc59-4e4a-a3bd-5679a8fcc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dfc502a-ecda-43e5-aeb2-9805f1df9cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f442dcaa-140f-46a2-a4b8-f69239e28320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250e762c-a242-4c3d-ac88-957c4bdef0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3de60008-40e2-4346-b53f-642b587fab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09df5160-316f-4533-8b40-720059247ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fae35af8-3daf-4e90-b1b1-9ebf46a1fdbf",
   "metadata": {},
   "source": [
    "### 3.3.2 Computing attention weights for all input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee5529fc-2e2b-44df-a577-c8dc398aab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)     \n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5b5bd15-2cf3-466a-964e-59658942bc23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9eeaf8c-72bd-4a52-bd15-40dd930a8f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1) # Normalization across the las dimension (columns)\n",
    "print(attn_weights)\n",
    "print(attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "501fad0c-fcef-425d-95af-518f330ccf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d371b890-2d27-4bb8-9913-bb12443de543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 6])\n",
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(attn_weights.shape)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5313416-1430-4cb7-bf1d-48984156d212",
   "metadata": {},
   "source": [
    "## 3.4 Implementing self-attention with trainable weights ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01f382a2-496e-414f-b301-c81a2d3eacb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4dd2516-3990-4dbb-9121-8309a57379f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "# requires_grad=False to reduce clutter in the outputs, but if we were to use\n",
    "# the weight matrices for model training, we would set requires_grad=True to update\n",
    "# these matrices during model training.\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "print(W_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b121fd-7d6e-47fa-bc56-fa44d004eb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa91306b-d721-4ba0-90cb-0b2a7e34cbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2603a4b0-6275-4917-b925-6204cc2de10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "677e960e-5502-426d-94c2-1c437b096eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f7087391-3c48-47c9-851c-390112413de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d57ddfc-2adb-40c8-96a3-1f4b8b7c631d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ec10b5-cee9-4d2c-883b-0bee40d39ec3",
   "metadata": {},
   "source": [
    "### 3.4.2 Implementing a compact SelfAttention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a26d173d-d68c-4b46-b8c3-898756c3d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d3cd007-205c-4e03-a354-6034580d0d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5837c213-8555-42d6-bd2a-dbbe3b9c54b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1362,  0.1853,  0.4083],\n",
       "        [ 0.1076,  0.1579,  0.5573]], requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = torch.nn.Linear(3, 2)\n",
    "m.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e49c7891-5354-4635-bd85-3427f7a87d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2775, 0.8993, 0.9268],\n",
       "        [0.8573, 0.0390, 0.7388]], grad_fn=<PermuteBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = nn.Parameter(torch.rand(3, 2))\n",
    "n.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7636177-b386-4cb9-85b1-ef72310b64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(\n",
    "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e55691e-21cc-4f2a-a289-5e531b01dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0556,  0.5242],\n",
      "        [-0.0550,  0.5207],\n",
      "        [-0.0551,  0.5208],\n",
      "        [-0.0557,  0.5226],\n",
      "        [-0.0559,  0.5243],\n",
      "        [-0.0553,  0.5213]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe03bab-4b9b-4e52-b802-95ac3e598d10",
   "metadata": {},
   "source": [
    "### _Exercise 3.1 Comparing SelfAttention_v1 and SelfAttention_v2_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d158ee01-6fc7-4ab0-b86f-103377466493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_Query_v1 wieghts:\n",
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]], requires_grad=True)\n",
      "W_Query_v2 wieghts:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2516,  0.2377,  0.4800],\n",
      "        [-0.0762, -0.4883, -0.1657]], requires_grad=True)\n",
      "Se puede observar que los pesos en sa_v1 son una matriz traspuesta de los pesos sa_v2 (se intercambian las dimensiones)\n"
     ]
    }
   ],
   "source": [
    "print(f\"W_Query_v1 wieghts:\\n{sa_v1.W_query}\")\n",
    "print(f\"W_Query_v2 wieghts:\\n{sa_v2.W_query.weight}\")\n",
    "print(\"Se puede observar que los pesos en sa_v1 son una matriz traspuesta de los pesos sa_v2 (se intercambian las dimensiones)\")\n",
    "with torch.no_grad():\n",
    "    sa_v1.W_query.copy_(sa_v2.W_query.weight.T)\n",
    "    sa_v1.W_key.copy_(sa_v2.W_key.weight.T)\n",
    "    sa_v1.W_value.copy_(sa_v2.W_value.weight.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb67d841-a852-4eec-a3b3-7bf87a4648a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0556,  0.5242],\n",
      "        [-0.0550,  0.5207],\n",
      "        [-0.0551,  0.5208],\n",
      "        [-0.0557,  0.5226],\n",
      "        [-0.0559,  0.5243],\n",
      "        [-0.0553,  0.5213]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b5c1af-dac9-4c83-a9fe-2e7109180ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5de70cf-e896-42a0-8434-d33eea1e0f18",
   "metadata": {},
   "source": [
    "## 3.5 Hiding future words with casual attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88e745c-2a6f-4941-9955-5d29660b6517",
   "metadata": {},
   "source": [
    "### 3.5.1 Applying a causal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68007141-e3bc-4b25-9adc-fa208766bca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([6, 2])\n",
      "Keys shape: torch.Size([6, 2])\n",
      "tensor([[0.1566, 0.1625, 0.1625, 0.1752, 0.1691, 0.1740],\n",
      "        [0.1481, 0.1610, 0.1615, 0.1779, 0.1805, 0.1709],\n",
      "        [0.1485, 0.1611, 0.1616, 0.1778, 0.1802, 0.1709],\n",
      "        [0.1554, 0.1636, 0.1640, 0.1729, 0.1759, 0.1683],\n",
      "        [0.1597, 0.1643, 0.1645, 0.1715, 0.1704, 0.1696],\n",
      "        [0.1515, 0.1625, 0.1630, 0.1750, 0.1796, 0.1685]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "print(\"Queries shape:\", queries.shape)\n",
    "print(\"Keys shape:\", keys.shape)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c6c888d-53ea-461a-b6ea-8fa0e0845921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fb8f31b-ee9a-4500-84d3-630b6c29d58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1566, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1481, 0.1610, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1485, 0.1611, 0.1616, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1554, 0.1636, 0.1640, 0.1729, 0.0000, 0.0000],\n",
      "        [0.1597, 0.1643, 0.1645, 0.1715, 0.1704, 0.0000],\n",
      "        [0.1515, 0.1625, 0.1630, 0.1750, 0.1796, 0.1685]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b613c1d4-dd0f-423f-82b7-5f838c9497de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4792, 0.5208, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3151, 0.3419, 0.3430, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2370, 0.2494, 0.2500, 0.2636, 0.0000, 0.0000],\n",
      "        [0.1923, 0.1979, 0.1981, 0.2065, 0.2052, 0.0000],\n",
      "        [0.1515, 0.1625, 0.1630, 0.1750, 0.1796, 0.1685]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3e065b9-b121-4142-98d4-51ce08e6005c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[-0.2390,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3746, -0.2567,    -inf,    -inf,    -inf,    -inf],\n",
      "        [-0.3682, -0.2526, -0.2479,    -inf,    -inf,    -inf],\n",
      "        [-0.2158, -0.1434, -0.1401, -0.0652,    -inf,    -inf],\n",
      "        [-0.1481, -0.1074, -0.1062, -0.0472, -0.0558,    -inf],\n",
      "        [-0.2916, -0.1925, -0.1878, -0.0878, -0.0511, -0.1411]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "print(mask)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "234445da-017f-475f-9117-5d7bc62a5288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4792, 0.5208, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3151, 0.3419, 0.3430, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2370, 0.2494, 0.2500, 0.2636, 0.0000, 0.0000],\n",
      "        [0.1923, 0.1979, 0.1981, 0.2065, 0.2052, 0.0000],\n",
      "        [0.1515, 0.1625, 0.1630, 0.1750, 0.1796, 0.1685]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4f9e1-fef2-4bd9-b9c1-7d7e2013bf22",
   "metadata": {},
   "source": [
    "### 3.5.2 Masking additional attention weights with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aecbce80-bda8-4b3e-b920-1452bf50bce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # 50% dropout\n",
    "example = torch.ones(6, 6)\n",
    "print(dropout(example)) #To compensate for the reduction inactive elements, the values of the remaining elements in the matrix are scaled up by afactor of 1/0.5 = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3481b308-b511-4c5d-9978-de25dff8c781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.6301, 0.6838, 0.6861, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4988, 0.5000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3958, 0.0000, 0.4130, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3250, 0.3260, 0.3499, 0.3591, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150ebb2-78a8-42cd-a427-17f7ec9efe2a",
   "metadata": {},
   "source": [
    "### 3.5.3 Implementing a compact causal attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ea32dba2-e5d2-418d-b5b6-881c06b22869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "89f00d5e-e3c4-4be6-98e9-0f3ca956940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        # x = batch, 2x6x3\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) #New, _ ops are in-place.\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c8944124-4f06-48d8-81a1-2e5f4a5715b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "print(batch.shape)\n",
    "dropout = 0.0\n",
    "ca = CausalAttention(d_in, d_out, context_length, dropout)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "962d3906-847b-4d81-bfca-b8f3df52085f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]],\n",
       "\n",
       "        [[-0.4519,  0.2216],\n",
       "         [-0.5874,  0.0058],\n",
       "         [-0.6300, -0.0632],\n",
       "         [-0.5675, -0.0843],\n",
       "         [-0.5526, -0.0981],\n",
       "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1af657-6b82-446b-a256-eba5e5f093a5",
   "metadata": {},
   "source": [
    "## 3.6 Extending single-head attention to multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac1c57-d39a-4569-a2d9-cf53748e5ada",
   "metadata": {},
   "source": [
    "### 3.6.1 Stacking multiple single-head attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df2b1d3c-bf8b-45ff-a684-cd1cc8a1b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a69dd06c-3bee-4509-803c-6f1bfb66c20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea613065-49f1-4e22-8770-7c341e5aa3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.0\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8868506-7191-452d-8917-80eba9ee97a1",
   "metadata": {},
   "source": [
    "### _Exercise 3.2 Returning two-dimensional embedding vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5d0d3604-7e39-4f35-af9b-e57fe4ab0d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in_ex, d_out_ex = 3, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1b507643-998e-4a26-b22f-64dc6bff292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_ex = MultiHeadAttentionWrapper(d_in_ex, d_out_ex, context_length, dropout, num_heads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2df89886-937e-4645-963b-f08bbeac2720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]],\n",
      "\n",
      "        [[0.0189, 0.2729],\n",
      "         [0.2181, 0.3037],\n",
      "         [0.2804, 0.3125],\n",
      "         [0.2830, 0.2793],\n",
      "         [0.2476, 0.2541],\n",
      "         [0.2748, 0.2513]]], grad_fn=<CatBackward0>)\n",
      "Exercise 3.2: context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "context_vecs = mha_ex(batch)\n",
    "print(context_vecs)\n",
    "print(\"Exercise 3.2: context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d18f2c-94f6-4bd5-99bd-595ff4b9dce4",
   "metadata": {},
   "source": [
    "### 3.6.2 Implementing multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f82c0c89-7bcb-4e8c-9c9e-7cb118e5c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7ae1b445-d0aa-43b8-9e54-717e4c85fc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
      "          [0.8993, 0.0390, 0.9268, 0.7388],\n",
      "          [0.7179, 0.7058, 0.9156, 0.4340]],\n",
      "\n",
      "         [[0.0772, 0.3565, 0.1479, 0.5331],\n",
      "          [0.4066, 0.2318, 0.4545, 0.9737],\n",
      "          [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
      "BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
      "torch.Size([1, 2, 4, 3])\n",
      "tensor([[[[0.2745, 0.8993, 0.7179],\n",
      "          [0.6584, 0.0390, 0.7058],\n",
      "          [0.2775, 0.9268, 0.9156],\n",
      "          [0.8573, 0.7388, 0.4340]],\n",
      "\n",
      "         [[0.0772, 0.4066, 0.4606],\n",
      "          [0.3565, 0.2318, 0.5159],\n",
      "          [0.1479, 0.4545, 0.4220],\n",
      "          [0.5331, 0.9737, 0.5786]]]])\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.3208, 1.1631, 1.2879],\n",
      "          [1.1631, 2.2150, 1.8424],\n",
      "          [1.2879, 1.8424, 2.0402]],\n",
      "\n",
      "         [[0.4391, 0.7003, 0.5903],\n",
      "          [0.7003, 1.3737, 1.0620],\n",
      "          [0.5903, 1.0620, 0.9912]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
    "[0.8993, 0.0390, 0.9268, 0.7388],\n",
    "[0.7179, 0.7058, 0.9156, 0.4340]],\n",
    "[[0.0772, 0.3565, 0.1479, 0.5331],\n",
    "[0.4066, 0.2318, 0.4545, 0.9737],\n",
    "[0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
    "print(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\")\n",
    "print(a.shape)\n",
    "print(a)\n",
    "print(\"BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\")\n",
    "b = a.transpose(2, 3)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "c = a @ b\n",
    "print(\"CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\")\n",
    "print(c.shape)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94a104bc-0a97-4839-a804-020191503c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
      "\n",
      "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
      "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
      "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
      "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
      "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
      "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 4\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ff75b7-d4f2-4244-b402-33fb63fde806",
   "metadata": {},
   "source": [
    "## _Exercise 3.3 Initializing GPT-2 size attention modules_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b2280210-7981-47bd-bdf8-c003d8c837a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_in_ex3_3 = 768\n",
    "d_out_ex3_3 = 768\n",
    "context_length_ex3_3 = 1024\n",
    "num_heads_ex3_3 = 12\n",
    "mha_ex3_3 = MultiHeadAttention(d_in_ex3_3, d_out_ex3_3, context_length_ex3_3, 0.0, num_heads_ex3_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308a3802-0f10-4ffb-ab85-b8c3bf2f506e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "432f2761-89eb-4cad-beac-6c43f7db4010",
   "metadata": {},
   "source": [
    "# 4. Implementing a GPT model from scratch to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef6bde-4400-4534-99f1-1feb6aecc972",
   "metadata": {},
   "source": [
    "## 4.1 Coding an LLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b21d90d-be20-4460-bf70-0122462b42cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768, # Embedding dimension\n",
    "    \"n_heads\": 12, # Number of attention heads\n",
    "    \"n_layers\": 12, # Number of layers\n",
    "    \"drop_rate\": 0.1, # Dropout rate\n",
    "    \"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "422cb3e0-510b-4db3-b0c4-ce6fdac1e2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]) # Uses a placeholder for TransformerBlock\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x) #!! Vector is projected in vocabulary space.\n",
    "        return logits\n",
    "        \n",
    "class DummyTransformerBlock(nn.Module): # A simple placeholder class that will be replaced by a real TransformerBlock later\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "class DummyLayerNorm(nn.Module): # A simple placeholder class that will be replaced by a real LayerNorm later\n",
    "    def __init__(self, normalized_shape, eps=1e-5): # The parameters here are just to mimic the LayerNorm interface.\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "30ea585d-0db2-4c04-861e-001b66dbe9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ceae0f40-aa5d-47b7-9495-64e4cb45bc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6528e-598f-4f53-8080-db2cfad32c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2689b32a-65a0-4b86-9b87-7cae43767f07",
   "metadata": {},
   "source": [
    "## 4.2 Normalizing activations with layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f8c7ec3-7cff-4541-a723-121ea44fc3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2, 5)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c0c4a027-63d8-4fa8-b090-dbedba1e513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "10879eba-7750-4219-b516-ff81320e844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8805372e-cb67-4aee-872c-593e2ad88fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.0000],\n",
      "        [0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False) # Turn off the scientific notation when printing tensor values\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a319fb74-2f38-45d4-b078-7b3b743642dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # Prevents division by zero during normalization.\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # Trainable parameter\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # Trainable parameter\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b5ca119-0e82-42d2-ab5c-5cfd10916e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[-0.0000],\n",
      "        [ 0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b24936-6bd6-4735-affb-b590e6ef1df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47721f0b-6ff4-4d55-adeb-c190181cbd31",
   "metadata": {},
   "source": [
    "## 4.3 Implementing a feed forward network with GELU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a8ba7215-721e-4c13-a9fa-75f74fcdd7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b0cb911f-6d42-4e01-8f13-c9309d43f52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-6, 6, 200)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e708329e-d7d8-43f0-b6fb-eef2b76190cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "        nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "        GELU(),\n",
    "        nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bb138c69-6ae4-4c0a-abf5-dad1eab536fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133d6ff-0d5b-4f96-ab42-b7d9c6f26f17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52253bd0-3485-4753-949c-cbbaf888f3ca",
   "metadata": {},
   "source": [
    "## 4.4 Adding shorcut connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "94ee1f30-e3aa-4d82-ab83-aa42c096e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]),\n",
    "            GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]),\n",
    "            GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]),\n",
    "            GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]),\n",
    "            GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]),\n",
    "            GELU())\n",
    "            ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1736631-0bdd-4eb7-a1d1-76ac6bce25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f1bc6cb-845b-4d5c-8baf-39e6cc989e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    loss.backward() #convenient method in PyTorch that computes loss gradients\n",
    "     \n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f00372ec-958c-4a15-8d3f-a1f5577aae53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169\n",
      "layers.1.0.weight has gradient mean of 0.00012011159560643137\n",
      "layers.2.0.weight has gradient mean of 0.0007152040489017963\n",
      "layers.3.0.weight has gradient mean of 0.0013988736318424344\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "22cb19a3-c1db-4886-819b-53c451415a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169798612594604\n",
      "layers.1.0.weight has gradient mean of 0.20694111287593842\n",
      "layers.2.0.weight has gradient mean of 0.3289700150489807\n",
      "layers.3.0.weight has gradient mean of 0.26657330989837646\n",
      "layers.4.0.weight has gradient mean of 1.3258544206619263\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(layer_sizes, use_shortcut=True)\n",
    "\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11b143-065d-45f5-bddc-57826a730c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff9d07d-ca8c-456b-8c40-a02c9df72719",
   "metadata": {},
   "source": [
    "## 4.5 Connecting attention and linear layers in a transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d348109-2f3b-46a0-a65f-2fd64670073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self, x):\n",
    "        shortcut = x #Shortcut connection for attention block\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Adds the original input back\n",
    "        \n",
    "        shortcut = x #Shortcut connection for feed forward block\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut # Adds the original input back\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4971a714-e2b2-498f-9064-fe07ac4fb47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "980ee046-267c-46cc-8d3a-d98fd3e96cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) #The device setting will allow us to train the model on a CPU or GPU, depending on which device the input data sits on.\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7bb8e101-507a-44f4-a37a-3a6e2d15855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[ 0.3613,  0.4222, -0.0711,  ...,  0.3483,  0.4661, -0.2838],\n",
      "         [-0.1792, -0.5660, -0.9485,  ...,  0.0477,  0.5181, -0.3168],\n",
      "         [ 0.7120,  0.0332,  0.1085,  ...,  0.1018, -0.4327, -0.2553],\n",
      "         [-1.0076,  0.3418, -0.1190,  ...,  0.7195,  0.4023,  0.0532]],\n",
      "\n",
      "        [[-0.2564,  0.0900,  0.0335,  ...,  0.2659,  0.4454, -0.6806],\n",
      "         [ 0.1230,  0.3653, -0.2074,  ...,  0.7705,  0.2710,  0.2246],\n",
      "         [ 1.0558,  1.0318, -0.2800,  ...,  0.6936,  0.3205, -0.3178],\n",
      "         [-0.1565,  0.3926,  0.3288,  ...,  1.2630, -0.1858,  0.0388]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1a8e1658-9a66-40e1-a18f-605ffda3058b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4fdc5e49-92aa-4f6f-bda8-4c8b6d37fcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3310c4cf-b96d-4f0e-a94a-c6776a4034ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 = (\n",
    "    total_params - sum(p.numel()\n",
    "    for p in model.out_head.parameters())\n",
    ")\n",
    "print(f\"Number of trainable parameters \"\n",
    "f\"considering weight tying: {total_params_gpt2:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5efa6715-290b-485c-b833-b48bc175b106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# Compute the memory requirements of the 163 million parameters in our GPTModel object\n",
    "total_size_bytes = total_params * 4 #Calculates the total size in bytes (assuming float32, 4 bytes per parameter)\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60563504-82a5-44a1-9c09-6c8089ee5f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "487c29b1-12da-4267-aaeb-efefc8005ff7",
   "metadata": {},
   "source": [
    "### Exercise 4.1 Number of parameters in feed forward and attention modules\n",
    "Calculate and compare the number of parameters that are contained in the feed for-\n",
    "ward module and those that are contained in the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "737c6f2a-92f3-4ae8-af2d-8198a0f64435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters Feed Forward module: 4722432\n",
      "Number of parameters MultiHead att module: 2360064\n"
     ]
    }
   ],
   "source": [
    "transforemer_block = TransformerBlock(GPT_CONFIG_124M)\n",
    "\n",
    "ff_params = sum(p.numel() for p in transforemer_block.ff.parameters())\n",
    "print(f\"Number of parameters Feed Forward module: {ff_params}\")\n",
    "att_params = sum(p.numel() for p in transforemer_block.att.parameters())\n",
    "print(f\"Number of parameters MultiHead att module: {att_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3572a202-7834-43e5-b1ee-ba3ce5a01703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
